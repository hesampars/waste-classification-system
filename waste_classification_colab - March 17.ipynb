{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOOiGEbs6wLv"
      },
      "source": [
        "# Waste Classification System - Google Colab Setup\n",
        "\n",
        "This notebook will guide you through setting up and running the waste classification system in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Beqg_iU6wLy"
      },
      "source": [
        "## 1. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the setup correction script\n",
        "!cp /content/drive/MyDrive/setup_correction.py /content/waste-classification-system/\n",
        "\n",
        "# Run the setup correction script\n",
        "%cd /content/waste-classification-system\n",
        "!python setup_correction.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdfsc5iHEf7g",
        "outputId": "58569b60-0b28-42d8-a684-f6a6273a3c1b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/waste-classification-system\n",
            "=== Waste Classification Project Setup Correction ===\n",
            "This script will create the necessary directory structure and move files to their proper locations.\n",
            "Current working directory: /content/waste-classification-system\n",
            "Creating directory structure...\n",
            "✓ Created directory: src\n",
            "✓ Created directory: scripts\n",
            "✓ Created directory: data\n",
            "✓ Created directory: models\n",
            "✓ Created directory: output\n",
            "\n",
            "Moving files to proper locations...\n",
            "✓ Moved classifier.py to src/classifier.py\n",
            "✓ Moved data_utils.py to src/data_utils.py\n",
            "✓ Moved detector.py to src/detector.py\n",
            "✓ Moved ensemble.py to src/ensemble.py\n",
            "✓ Moved download_datasets.py to scripts/download_datasets.py\n",
            "✓ Moved preprocess_datasets.py to scripts/preprocess_datasets.py\n",
            "✓ Moved train.py to scripts/train.py\n",
            "✗ File not found: fixed_download_datasets.py\n",
            "✓ Moved colab_pro_download_datasets.py to scripts/colab_pro_download_datasets.py\n",
            "\n",
            "Creating __init__.py files...\n",
            "✓ Created src/__init__.py\n",
            "✓ Created scripts/__init__.py\n",
            "\n",
            "Updating import paths...\n",
            "✓ Updated import paths in src/classifier.py\n",
            "✓ Updated import paths in src/detector.py\n",
            "✓ Updated import paths in src/ensemble.py\n",
            "✓ Updated import paths in scripts/download_datasets.py\n",
            "✓ Updated import paths in scripts/preprocess_datasets.py\n",
            "✓ Updated import paths in scripts/train.py\n",
            "✓ Updated import paths in app.py\n",
            "\n",
            "Verifying setup...\n",
            "✓ Directory exists: src\n",
            "✓ Directory exists: scripts\n",
            "✓ Directory exists: data\n",
            "✓ Directory exists: models\n",
            "✓ Directory exists: output\n",
            "✓ File exists: src/classifier.py\n",
            "✓ File exists: src/data_utils.py\n",
            "✓ File exists: src/detector.py\n",
            "✓ File exists: src/ensemble.py\n",
            "✓ File exists: scripts/download_datasets.py\n",
            "✓ File exists: scripts/preprocess_datasets.py\n",
            "✓ File exists: scripts/train.py\n",
            "✓ File exists: app.py\n",
            "✓ File exists: config.py\n",
            "\n",
            "=== Setup correction completed ===\n",
            "You can now proceed with the waste classification project as described in the guide.\n",
            "If any files are missing, please check the error messages above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgHJQaU46wLy",
        "outputId": "54001ca4-f53f-4b8b-a3fd-b6f2d80149f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.92-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.28.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.92-py3-none-any.whl (949 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.3/949.3 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markupsafe, groovy, ffmpy, aiofiles, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.0 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 ultralytics-8.3.92 ultralytics-thop-2.0.14 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision timm numpy pillow opencv-python matplotlib scikit-learn tqdm requests gradio ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKbT4v9t6wLz"
      },
      "source": [
        "## 2. Clone the Repository\n",
        "\n",
        "Make sure you've created your GitHub repository and uploaded the project files as instructed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0-Jxv_b6wL0",
        "outputId": "c32e1da0-c344-41e6-ae28-f43c51014ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'waste-classification-system'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 23 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (23/23), 23.85 KiB | 23.85 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "/content/waste-classification-system\n"
          ]
        }
      ],
      "source": [
        "# Clone your repository (replace with your actual repository URL)\n",
        "!git clone https://github.com/hesampars/waste-classification-system.git\n",
        "%cd waste-classification-system\n",
        "\n",
        "# Create necessary directories\n",
        "!mkdir -p data models output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Gsl4jy6wL0"
      },
      "source": [
        "## 3. Mount Google Drive\n",
        "\n",
        "We'll mount your Google Drive to access the dataset zip files you've manually downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO3OOVsy6wL1",
        "outputId": "2fc3d90e-241f-43cf-c34b-7dc1f6168c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R8iKGyT6wL1"
      },
      "source": [
        "## 4. Upload Datasets to Google Drive\n",
        "\n",
        "Before running the next cell, make sure you've:\n",
        "1. Created a folder in your Google Drive (e.g., 'waste_datasets')\n",
        "2. Uploaded your dataset zip files to this folder:\n",
        "   - MJU-Waste.zip\n",
        "   - TACO-master.zip\n",
        "   - trashnet-master.zip\n",
        "   - waste-pictures.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxRGX9Q16wL2",
        "outputId": "b9e3a825-b7ab-4200-ab88-d31cf545380b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3640656\n",
            "drwxr-xr-x 2 root root       4096 Mar 17 21:18 .\n",
            "drwxr-xr-x 6 root root       4096 Mar 17 21:15 ..\n",
            "-rw------- 1 root root 1446750945 Mar 17 21:17 MJU-Waste.zip\n",
            "-rw------- 1 root root   38113038 Mar 17 21:18 TACO-master.zip\n",
            "-rw------- 1 root root   42609527 Mar 17 21:18 trashnet-master.zip\n",
            "-rw------- 1 root root 2200536115 Mar 17 21:18 waste-pictures.zip\n"
          ]
        }
      ],
      "source": [
        "# Create a directory for the datasets\n",
        "!mkdir -p data\n",
        "\n",
        "# Copy datasets from Google Drive to the project\n",
        "# Adjust the path if your folder structure is different\n",
        "!cp /content/drive/MyDrive/waste_datasets/*.zip data/\n",
        "\n",
        "# List the copied files\n",
        "!ls -la data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2PgU5v6wL2"
      },
      "source": [
        "## 5. Extract and Process Datasets\n",
        "\n",
        "Now we'll extract the datasets and prepare them for training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/colab_pro_download_datasets.py /content/waste-classification-system/\n"
      ],
      "metadata": {
        "id": "9ijVjM3QAEph"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix import paths in download_datasets.py\n",
        "with open('/content/waste-classification-system/scripts/improved_download_datasets.py', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Replace the import statement\n",
        "content = content.replace(\"import improved_config as config\",\n",
        "                         \"import sys\\nsys.path.append('/content/waste-classification-system')\\nimport improved_config as config\")\n",
        "\n",
        "with open('/content/waste-classification-system/scripts/improved_download_datasets.py', 'w') as file:\n",
        "    file.write(content)\n",
        "\n",
        "# Fix import paths in preprocess_datasets.py\n",
        "with open('/content/waste-classification-system/scripts/improved_preprocess_datasets.py', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Replace the import statement\n",
        "content = content.replace(\"import improved_config as config\",\n",
        "                         \"import sys\\nsys.path.append('/content/waste-classification-system')\\nimport improved_config as config\")\n",
        "\n",
        "with open('/content/waste-classification-system/scripts/improved_preprocess_datasets.py', 'w') as file:\n",
        "    file.write(content)\n",
        "\n",
        "print(\"Import paths fixed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1obrpwwqpOGC",
        "outputId": "9157fe78-e865-4a19-fb3e-5ba31f2ed1ca"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import paths fixed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download datasets using improved script\n",
        "!python /content/waste-classification-system/scripts/improved_download_datasets.py --gdrive /content/drive --colab-pro\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfG_VtR5pro1",
        "outputId": "9b9d43e7-8260-4a7f-d580-6baad71cb8d7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using trashnet zip from Google Drive: /content/drive/MyDrive/waste_datasets/trashnet-master.zip\n",
            "Using taco zip from Google Drive: /content/drive/MyDrive/waste_datasets/TACO-master.zip\n",
            "Using waste_pictures zip from Google Drive: /content/drive/MyDrive/waste_datasets/waste-pictures.zip\n",
            "Using mju_waste zip from Google Drive: /content/drive/MyDrive/waste_datasets/MJU-Waste.zip\n",
            "🔄 Updated paths for Google Drive mounted at /content/drive\n",
            "\n",
            "==================================================\n",
            "📥 Downloading trashnet dataset\n",
            "==================================================\n",
            "📁 Using manually downloaded TrashNet zip: /content/drive/MyDrive/waste_datasets/trashnet-master.zip\n",
            "📦 Extracting trashnet-master.zip to /content/waste-classification-system/data/trashnet...\n",
            "❌ Error: Main directory not found at /content/waste-classification-system/data/trashnet/trashnet-master/dataset\n",
            "🔍 Searching for possible main directory...\n",
            "💡 Found possible main directory at: /content/waste-classification-system/data/trashnet/trashnet-master/data/dataset/dataset-resized\n",
            "\n",
            "==================================================\n",
            "📥 Downloading taco dataset\n",
            "==================================================\n",
            "📁 Using manually downloaded TACO zip: /content/drive/MyDrive/waste_datasets/TACO-master.zip\n",
            "📦 Extracting TACO-master.zip to /content/waste-classification-system/data/taco...\n",
            "\n",
            "==================================================\n",
            "📥 Downloading waste_pictures dataset\n",
            "==================================================\n",
            "📁 Using manually downloaded Waste-Pictures zip: /content/drive/MyDrive/waste_datasets/waste-pictures.zip\n",
            "📦 Extracting waste-pictures.zip to /content/waste-classification-system/data/waste-pictures...\n",
            "❌ Error: Main directory not found at /content/waste-classification-system/data/waste-pictures/waste-pictures-master\n",
            "🔍 Searching for possible main directory...\n",
            "⚠️ Failed to extract manually downloaded zip, trying to download...\n",
            "❌ Error downloading https://github.com/AgaMiko/waste-pictures/archive/master.zip: 404 Client Error: Not Found for url: https://github.com/AgaMiko/waste-pictures/archive/master.zip\n",
            "\n",
            "==================================================\n",
            "📥 Downloading mju_waste dataset\n",
            "==================================================\n",
            "📁 Using manually downloaded MJU-Waste zip: /content/drive/MyDrive/waste_datasets/MJU-Waste.zip\n",
            "📦 Extracting MJU-Waste.zip to /content/waste-classification-system/data/mju-waste...\n",
            "⚠️ Warning: Class directory general-waste not found at /content/waste-classification-system/data/mju-waste/general-waste\n",
            "\n",
            "==================================================\n",
            "📥 Downloading open_images dataset\n",
            "==================================================\n",
            "🚀 A100 GPU detected! Optimizing for maximum performance...\n",
            "📥 Downloading Open Images for class: Bottle, type: train\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Bottle.\u001b[0m\n",
            "\n",
            "\u001b[95mBottle\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 11456 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | Download of 2000 images in train.\u001b[0m\n",
            "100% 2000/2000 [05:36<00:00,  5.94it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Bottle of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "📥 Downloading Open Images for class: Bottle, type: validation\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Bottle.\u001b[0m\n",
            "\n",
            "\u001b[95mBottle\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 177 online images for validation.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | Download of 177 images in validation.\u001b[0m\n",
            "100% 177/177 [00:34<00:00,  5.09it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Bottle of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "❌ Missing validation-annotations-bbox.csv file\n",
            "🔧 Attempting to fix by downloading the file directly...\n",
            "Downloading validation-annotations-bbox.csv: 100% 2.10G/2.10G [01:36<00:00, 23.5MiB/s]\n",
            "✅ Successfully downloaded validation-annotations-bbox.csv\n",
            "📥 Downloading Open Images for class: Tin can, type: train\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Tin can.\u001b[0m\n",
            "\n",
            "\u001b[95mTin can\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 1307 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | Download of 1307 images in train.\u001b[0m\n",
            " 77% 1012/1307 [02:50<00:34,  8.61it/s]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/aws\", line 27, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/aws\", line 27, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/bin/aws\", line 23, in main\n",
            "    return awscli.clidriver.main()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 73, in main\n",
            "    sys.exit(main())\n",
            "    driver = create_clidriver()\n",
            "             ^^^^^^\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "fatal error: \n",
            "  File \"/usr/local/bin/aws\", line 23, in main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 82, in create_clidriver\n",
            "Traceback (most recent call last):\n",
            "    return awscli.clidriver.main()\n",
            "    load_plugins(\n",
            "  File \"/usr/local/bin/aws\", line 27, in <module>\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/plugin.py\", line 44, in load_plugins\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 73, in main\n",
            "    modules = _import_plugins(plugin_mapping)\n",
            "fatal error: \n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/plugin.py\", line 61, in _import_plugins\n",
            "    driver = create_clidriver()\n",
            "fatal error: \n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 82, in create_clidriver\n",
            "    module = __import__(path, fromlist=[module])\n",
            "    sys.exit(main())\n",
            "fatal error: \n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/handlers.py\", line 27, in <module>\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/bin/aws\", line 23, in main\n",
            "    load_plugins(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/plugin.py\", line 44, in load_plugins\n",
            "    from awscli.customizations.cloudformation import (\n",
            "    return awscli.clidriver.main()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/customizations/cloudformation/__init__.py\", line 13, in <module>\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    modules = _import_plugins(plugin_mapping)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 74, in main\n",
            "    from awscli.customizations.cloudformation.package import PackageCommand\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/plugin.py\", line 61, in _import_plugins\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/customizations/cloudformation/package.py\", line 22, in <module>\n",
            "cancelled: ctrl-c received\n",
            "    rc = driver.main()\n",
            "         ^^^^^^^^^^^^^\n",
            "    from awscli.customizations.cloudformation.artifact_exporter import Template\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 219, in main\n",
            "    module = __import__(path, fromlist=[module])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/customizations/cloudformation/artifact_exporter.py\", line 25, in <module>\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/handlers.py\", line 27, in <module>\n",
            "    from awscli.customizations.cloudformation import exceptions\n",
            "    command_table = self._get_command_table()\n",
            "    from awscli.customizations.cloudformation import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/customizations/cloudformation/__init__.py\", line 13, in <module>\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 118, in _get_command_table\n",
            "  File \"<frozen importlib._bootstrap>\", line 701, in _load_unlocked\n",
            "KeyboardInterrupt\n",
            "    from awscli.customizations.cloudformation.package import PackageCommand\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/customizations/cloudformation/package.py\", line 22, in <module>\n",
            "    self._command_table = self._build_command_table()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1264, in wait\n",
            "    from awscli.customizations.cloudformation.artifact_exporter import Template\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 135, in _build_command_table\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/customizations/cloudformation/artifact_exporter.py\", line 26, in <module>\n",
            "    from awscli.customizations.cloudformation.yamlhelper import yaml_dump, \\\n",
            "    self.session.emit(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/customizations/cloudformation/yamlhelper.py\", line 16, in <module>\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/session.py\", line 800, in emit\n",
            "    import yaml\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/__init__.py\", line 8, in <module>\n",
            "    from .loader import *\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/loader.py\", line 9, in <module>\n",
            "    from .resolver import *\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/resolver.py\", line 188, in <module>\n",
            "    return self._wait(timeout=timeout)\n",
            "    re.compile(r'''^(?:[-+]?0b[0-1_]+\n",
            "  File \"/usr/lib/python3.11/re/__init__.py\", line 227, in compile\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2053, in _wait\n",
            "    return self._events.emit(event_name, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/hooks.py\", line 412, in emit\n",
            "    return self._emitter.emit(aliased_event_name, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/hooks.py\", line 256, in emit\n",
            "    return self._emit(event_name, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/hooks.py\", line 239, in _emit\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "    response = handler(**kwargs)\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2011, in _try_wait\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/customizations/preview.py\", line 69, in mark_as_preview\n",
            "    service_name=original_command.service_model.service_name,\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 354, in service_model\n",
            "    return self._get_service_model()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/awscli/clidriver.py\", line 377, in _get_service_model\n",
            "    self._service_model = self.session.get_service_model(\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/session.py\", line 597, in get_service_model\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/waste-classification-system/scripts/improved_download_datasets.py\", line 610, in <module>\n",
            "    service_description = self.get_service_data(service_name, api_version)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/session.py\", line 619, in get_service_data\n",
            "    return _compile(pattern, flags)\n",
            "    service_data = self.get_component('data_loader').load_service_model(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/loaders.py\", line 143, in _wrapper\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/__init__.py\", line 294, in _compile\n",
            "    main()\n",
            "  File \"/content/waste-classification-system/scripts/improved_download_datasets.py\", line 584, in main\n",
            "    data = func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/loaders.py\", line 418, in load_service_model\n",
            "    p = _compiler.compile(pattern, flags)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/_compiler.py\", line 745, in compile\n",
            "    model = self.load_data(full_path)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/loaders.py\", line 472, in load_data\n",
            "    dataset_paths[dataset] = download_open_images(\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/waste-classification-system/scripts/improved_download_datasets.py\", line 457, in download_open_images\n",
            "    p = _parser.parse(p, flags)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    data, _ = self.load_data_with_path(name)\n",
            "  File \"/usr/lib/python3.11/re/_parser.py\", line 989, in parse\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/loaders.py\", line 143, in _wrapper\n",
            "    data = func(self, *args, **kwargs)\n",
            "    subprocess.check_call([\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 408, in check_call\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/loaders.py\", line 449, in load_data_with_path\n",
            "    retcode = call(*popenargs, **kwargs)\n",
            "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 391, in call\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    found = self.file_loader.load_file(possible_path)\n",
            "  File \"/usr/lib/python3.11/re/_parser.py\", line 464, in _parse_sub\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/loaders.py\", line 195, in load_file\n",
            "    return p.wait(timeout=timeout)\n",
            "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1277, in wait\n",
            "    data = self._load_file(file_path + ext, open_method)\n",
            "  File \"/usr/lib/python3.11/re/_parser.py\", line 872, in _parse\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/botocore/loaders.py\", line 182, in _load_file\n",
            "    return json.loads(payload, object_pairs_hook=OrderedDict)\n",
            "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 359, in loads\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.11/re/_parser.py\", line 464, in _parse_sub\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2047, in _wait\n",
            "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    return cls(**kw).decode(s)\n",
            "  File \"/usr/lib/python3.11/re/_parser.py\", line 872, in _parse\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n",
            "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
            "    time.sleep(delay)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/_parser.py\", line 464, in _parse_sub\n",
            "KeyboardInterrupt\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 353, in raw_decode\n",
            "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/_parser.py\", line 613, in _parse\n",
            "    obj, end = self.scan_once(s, idx)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "    code2 = LITERAL, _ord(that)\n",
            "                     ^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "cancelled: ctrl-c received\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the A100-optimized scripts\n",
        "!cp /content/drive/MyDrive/a100_optimized_download_datasets.py /content/waste-classification-system/\n",
        "\n",
        "!python /content/waste-classification-system/a100_optimized_download_datasets.py --colab-pro --optimize-a100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IeRrBq_MVDQ",
        "outputId": "90f75c8c-0b8b-4dc8-8d78-ce3781416bfb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A100 GPU detected! Applying optimizations...\n",
            "\n",
            "A100 GPU Optimization Settings:\n",
            "- batch_size: 64\n",
            "- image_size: 384\n",
            "- mixed_precision: True\n",
            "- learning_rate: 0.0005\n",
            "- optimizer: AdamW\n",
            "- weight_decay: 0.01\n",
            "- gradient_accumulation_steps: 2\n",
            "- Using enhanced model variants for A100 GPU\n",
            "Using manually downloaded TrashNet zip: data/trashnet-master.zip\n",
            "Using manually downloaded TACO zip: data/TACO-master.zip\n",
            "Using manually downloaded Waste-Pictures zip: data/waste-pictures.zip\n",
            "Using manually downloaded MJU-Waste zip: data/MJU-Waste.zip\n",
            "A100 GPU detected! Optimizing for maximum performance...\n",
            "Downloading Open Images for class: Bottle, type: train\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Bottle.\u001b[0m\n",
            "\n",
            "\u001b[95mBottle\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 11456 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | Download of 1588 images in train.\u001b[0m\n",
            "100% 1588/1588 [04:36<00:00,  5.75it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Bottle of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "Downloading Open Images for class: Bottle, type: validation\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Bottle.\u001b[0m\n",
            "\n",
            "\u001b[95mBottle\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 177 online images for validation.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | All images already downloaded.\u001b[0m\n",
            "    [INFO] | Creating labels for Bottle of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "Downloading Open Images for class: Tin can, type: train\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Tin can.\u001b[0m\n",
            "\n",
            "\u001b[95mTin can\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 1307 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | All images already downloaded.\u001b[0m\n",
            "    [INFO] | Creating labels for Tin can of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "Downloading Open Images for class: Tin can, type: validation\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Tin can.\u001b[0m\n",
            "\n",
            "\u001b[95mTin can\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 31 online images for validation.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | All images already downloaded.\u001b[0m\n",
            "    [INFO] | Creating labels for Tin can of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "Downloading Open Images for class: Box, type: train\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Box.\u001b[0m\n",
            "\n",
            "\u001b[95mBox\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 2212 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | Download of 191 images in train.\u001b[0m\n",
            "100% 191/191 [00:36<00:00,  5.16it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Box of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "Downloading Open Images for class: Box, type: validation\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Box.\u001b[0m\n",
            "\n",
            "\u001b[95mBox\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 61 online images for validation.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | All images already downloaded.\u001b[0m\n",
            "    [INFO] | Creating labels for Box of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "Downloading Open Images for class: Plastic bag, type: train\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Plastic bag.\u001b[0m\n",
            "\n",
            "\u001b[95mPlastic bag\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 517 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | All images already downloaded.\u001b[0m\n",
            "    [INFO] | Creating labels for Plastic bag of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "Downloading Open Images for class: Plastic bag, type: validation\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Plastic bag.\u001b[0m\n",
            "\n",
            "\u001b[95mPlastic bag\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 9 online images for validation.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | All images already downloaded.\u001b[0m\n",
            "    [INFO] | Creating labels for Plastic bag of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "Downloading Open Images for class: Food, type: train\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Food.\u001b[0m\n",
            "\n",
            "\u001b[95mFood\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 25712 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | Download of 1840 images in train.\u001b[0m\n",
            "100% 1840/1840 [05:19<00:00,  5.76it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Food of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "Downloading Open Images for class: Food, type: validation\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Food.\u001b[0m\n",
            "\n",
            "\u001b[95mFood\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 1800 online images for validation.\u001b[0m\n",
            "    [INFO] | Limiting to 2000 images.\u001b[0m\n",
            "    [INFO] | All images already downloaded.\u001b[0m\n",
            "    [INFO] | Creating labels for Food of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "\n",
            "Dataset download summary:\n",
            "✅ trashnet: data/trashnet/trashnet-master/dataset\n",
            "✅ taco: data/taco/TACO-master/data\n",
            "✅ waste-pictures: data/waste-pictures\n",
            "✅ mju-waste: data/mju-waste\n",
            "✅ open-images: data/open-images\n",
            "\n",
            "Colab Pro Optimizations:\n",
            "- Open Images limit per class: 2000\n",
            "- Using verified class names for Open Images\n",
            "- Both training and validation sets downloaded\n",
            "- A100 GPU optimizations applied\n",
            "- Enhanced model variants configured\n",
            "- Optimization settings saved to data/a100_optimizations.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wIBrNdgA6wL3"
      },
      "outputs": [],
      "source": [
        "# Download Open Images dataset (this may take some time)\n",
        "# Uncomment if you want to download Open Images\n",
        "# !python scripts/download_datasets.py --datasets open-images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/waste-classification-system/scripts/preprocess_datasets.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tpj02CdQRKk",
        "outputId": "e05a7a14-0965-4677-92aa-85f6c015d2dc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing TrashNet dataset from: /content/waste-classification-system/data/trashnet/trashnet-master/data/dataset/dataset-resized\n",
            "Processed 2527 images from TrashNet\n",
            "Processing TACO dataset from: /content/waste-classification-system/data/taco/TACO-master/data\n",
            "Processed 0 images from TACO\n",
            "Processing MJU-Waste dataset from: /content/waste-classification-system/data/mju-waste/simplified\n",
            "Processed 0 images from MJU-Waste\n",
            "Processing Waste-Pictures dataset from: /content/waste-classification-system/data/waste-pictures/train\n",
            "Processed 17872 images from Waste-Pictures\n",
            "Processing Open Images dataset from: /content/waste-classification-system/data/open-images\n",
            "Processed 4 images from Open Images\n",
            "Total processed images: 20403\n",
            "Created dataset splits: 14279 train, 3058 val, 3066 test\n",
            "Saved splits to: /content/waste-classification-system/data/processed/splits.json\n",
            "Dataset preprocessing completed successfully\n",
            "\n",
            "Class distribution in training set:\n",
            "  trash: 12021 images\n",
            "  e-waste: 587 images\n",
            "  paper: 415 images\n",
            "  glass: 350 images\n",
            "  plastic: 337 images\n",
            "  metal: 287 images\n",
            "  cardboard: 282 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code will update both data_utils.py files with the fixed version\n",
        "\n",
        "# Define the fixed code\n",
        "fixed_code = \"\"\"#!/usr/bin/env python3\n",
        "\\\"\\\"\\\"\n",
        "Data utilities module for waste classification system.\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Import local modules\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "import config\n",
        "\n",
        "class WasteDatasetPreprocessor:\n",
        "    \\\"\\\"\\\"Preprocessor for waste classification datasets.\\\"\\\"\\\"\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        \\\"\\\"\\\"\n",
        "        Initialize the dataset preprocessor.\n",
        "\n",
        "        Args:\n",
        "            data_dir: Directory containing the datasets\n",
        "        \\\"\\\"\\\"\n",
        "        self.data_dir = data_dir\n",
        "        self.output_dir = os.path.join(data_dir, \"processed\")\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        # Class mapping for standardization\n",
        "        self.class_mapping = {\n",
        "            # TrashNet mapping\n",
        "            \"glass\": \"glass\",\n",
        "            \"paper\": \"paper\",\n",
        "            \"cardboard\": \"cardboard\",\n",
        "            \"plastic\": \"plastic\",\n",
        "            \"metal\": \"metal\",\n",
        "            \"trash\": \"trash\",\n",
        "\n",
        "            # TACO mapping\n",
        "            \"Plastic bottle\": \"plastic\",\n",
        "            \"Bottle cap\": \"plastic\",\n",
        "            \"Plastic bag & wrapper\": \"plastic\",\n",
        "            \"Carton\": \"cardboard\",\n",
        "            \"Paper\": \"paper\",\n",
        "            \"Aluminium foil\": \"metal\",\n",
        "            \"Metal can\": \"metal\",\n",
        "            \"Glass bottle\": \"glass\",\n",
        "            \"Plastic container\": \"plastic\",\n",
        "            \"Plastic utensils\": \"plastic\",\n",
        "            \"Pop tab\": \"metal\",\n",
        "            \"Straw\": \"plastic\",\n",
        "            \"Paper cup\": \"paper\",\n",
        "            \"Plastic cup\": \"plastic\",\n",
        "            \"Plastic lid\": \"plastic\",\n",
        "            \"Cigarette\": \"trash\",\n",
        "            \"Other plastic\": \"plastic\",\n",
        "            \"Other metal\": \"metal\",\n",
        "            \"Other glass\": \"glass\",\n",
        "            \"Other paper\": \"paper\",\n",
        "            \"Unlabeled litter\": \"trash\",\n",
        "\n",
        "            # MJU-Waste mapping\n",
        "            \"battery\": \"e-waste\",\n",
        "            \"biological\": \"organic\",\n",
        "            \"brown-glass\": \"glass\",\n",
        "            \"cardboard\": \"cardboard\",\n",
        "            \"clothes\": \"textile\",\n",
        "            \"green-glass\": \"glass\",\n",
        "            \"metal\": \"metal\",\n",
        "            \"paper\": \"paper\",\n",
        "            \"plastic\": \"plastic\",\n",
        "            \"shoes\": \"textile\",\n",
        "            \"trash\": \"trash\",\n",
        "            \"white-glass\": \"glass\",\n",
        "\n",
        "            # Waste-Pictures mapping\n",
        "            \"battery\": \"e-waste\",\n",
        "            \"biological\": \"organic\",\n",
        "            \"clothes\": \"textile\",\n",
        "            \"e-waste\": \"e-waste\",\n",
        "            \"glass\": \"glass\",\n",
        "            \"metal\": \"metal\",\n",
        "            \"paper\": \"paper\",\n",
        "            \"plastic\": \"plastic\",\n",
        "            \"textile\": \"textile\",\n",
        "            \"trash\": \"trash\",\n",
        "            \"mixed\": \"mixed\",\n",
        "\n",
        "            # Open Images mapping\n",
        "            \"Bottle\": \"plastic\",\n",
        "            \"Tin can\": \"metal\",\n",
        "            \"Plastic bag\": \"plastic\",\n",
        "            \"Cardboard\": \"cardboard\",\n",
        "            \"Paper\": \"paper\",\n",
        "            \"Glass\": \"glass\",\n",
        "            \"Mobile phone\": \"e-waste\",\n",
        "            \"Computer\": \"e-waste\",\n",
        "            \"Food\": \"organic\",\n",
        "            \"Clothing\": \"textile\"\n",
        "        }\n",
        "\n",
        "    def preprocess_trashnet(self):\n",
        "        \\\"\\\"\\\"\n",
        "        Preprocess TrashNet dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of processed image metadata\n",
        "        \\\"\\\"\\\"\n",
        "        dataset_dir = os.path.join(self.data_dir, \"trashnet\")\n",
        "        if not os.path.exists(dataset_dir):\n",
        "            print(f\"TrashNet dataset not found at: {dataset_dir}\")\n",
        "            return []\n",
        "\n",
        "        # Find the dataset folder (might be nested)\n",
        "        data_folder = None\n",
        "        for root, dirs, files in os.walk(dataset_dir):\n",
        "            if \"glass\" in dirs and \"paper\" in dirs and \"cardboard\" in dirs:\n",
        "                data_folder = root\n",
        "                break\n",
        "\n",
        "        if not data_folder:\n",
        "            print(\"Could not find TrashNet data folder structure\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Processing TrashNet dataset from: {data_folder}\")\n",
        "\n",
        "        metadata = []\n",
        "\n",
        "        # Process each class folder\n",
        "        for class_name in os.listdir(data_folder):\n",
        "            class_dir = os.path.join(data_folder, class_name)\n",
        "\n",
        "            if not os.path.isdir(class_dir):\n",
        "                continue\n",
        "\n",
        "            # Map class name\n",
        "            mapped_class = self.class_mapping.get(class_name, \"trash\")\n",
        "\n",
        "            # Process images in this class\n",
        "            for filename in os.listdir(class_dir):\n",
        "                if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    continue\n",
        "\n",
        "                src_path = os.path.join(class_dir, filename)\n",
        "                dst_path = os.path.join(self.output_dir, f\"trashnet_{filename}\")\n",
        "\n",
        "                # Copy file to output directory\n",
        "                shutil.copy(src_path, dst_path)\n",
        "\n",
        "                # Add metadata\n",
        "                metadata.append({\n",
        "                    \"file\": os.path.basename(dst_path),\n",
        "                    \"source\": \"trashnet\",\n",
        "                    \"original_class\": class_name,\n",
        "                    \"class\": mapped_class\n",
        "                })\n",
        "\n",
        "        print(f\"Processed {len(metadata)} images from TrashNet\")\n",
        "        return metadata\n",
        "\n",
        "    def preprocess_taco(self):\n",
        "        \\\"\\\"\\\"\n",
        "        Preprocess TACO dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of processed image metadata\n",
        "        \\\"\\\"\\\"\n",
        "        dataset_dir = os.path.join(self.data_dir, \"taco\")\n",
        "        if not os.path.exists(dataset_dir):\n",
        "            print(f\"TACO dataset not found at: {dataset_dir}\")\n",
        "            return []\n",
        "\n",
        "        # Find the annotations file\n",
        "        annotations_file = None\n",
        "        for root, dirs, files in os.walk(dataset_dir):\n",
        "            if \"annotations.json\" in files:\n",
        "                annotations_file = os.path.join(root, \"annotations.json\")\n",
        "                break\n",
        "\n",
        "        if not annotations_file:\n",
        "            print(\"Could not find TACO annotations.json file\")\n",
        "            return []\n",
        "\n",
        "        # Load annotations\n",
        "        try:\n",
        "            with open(annotations_file, 'r') as f:\n",
        "                annotations = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading TACO annotations: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Processing TACO dataset from: {os.path.dirname(annotations_file)}\")\n",
        "\n",
        "        metadata = []\n",
        "\n",
        "        # Get image directory\n",
        "        image_dir = os.path.join(os.path.dirname(annotations_file), \"data\")\n",
        "        if not os.path.exists(image_dir):\n",
        "            image_dir = os.path.dirname(annotations_file)\n",
        "\n",
        "        # Process images\n",
        "        for image_info in annotations[\"images\"]:\n",
        "            image_id = image_info[\"id\"]\n",
        "            filename = image_info[\"file_name\"]\n",
        "\n",
        "            # Find annotations for this image\n",
        "            image_annotations = [a for a in annotations[\"annotations\"] if a[\"image_id\"] == image_id]\n",
        "\n",
        "            if not image_annotations:\n",
        "                continue\n",
        "\n",
        "            # Get most common category\n",
        "            category_counts = {}\n",
        "            for ann in image_annotations:\n",
        "                category_id = ann[\"category_id\"]\n",
        "                category_info = next((c for c in annotations[\"categories\"] if c[\"id\"] == category_id), None)\n",
        "\n",
        "                if category_info:\n",
        "                    category_name = category_info[\"name\"]\n",
        "                    category_counts[category_name] = category_counts.get(category_name, 0) + 1\n",
        "\n",
        "            if not category_counts:\n",
        "                continue\n",
        "\n",
        "            # Get most common category\n",
        "            original_class = max(category_counts.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "            # Map class name\n",
        "            mapped_class = self.class_mapping.get(original_class, \"trash\")\n",
        "\n",
        "            # Source and destination paths\n",
        "            src_path = os.path.join(image_dir, filename)\n",
        "            dst_path = os.path.join(self.output_dir, f\"taco_{os.path.basename(filename)}\")\n",
        "\n",
        "            # Check if source file exists\n",
        "            if not os.path.exists(src_path):\n",
        "                continue\n",
        "\n",
        "            # Copy file to output directory\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "            # Add metadata\n",
        "            metadata.append({\n",
        "                \"file\": os.path.basename(dst_path),\n",
        "                \"source\": \"taco\",\n",
        "                \"original_class\": original_class,\n",
        "                \"class\": mapped_class\n",
        "            })\n",
        "\n",
        "        print(f\"Processed {len(metadata)} images from TACO\")\n",
        "        return metadata\n",
        "\n",
        "    def preprocess_mju_waste(self):\n",
        "        \\\"\\\"\\\"\n",
        "        Preprocess MJU-Waste dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of processed image metadata\n",
        "        \\\"\\\"\\\"\n",
        "        dataset_dir = os.path.join(self.data_dir, \"mju-waste\")\n",
        "        if not os.path.exists(dataset_dir):\n",
        "            print(f\"MJU-Waste dataset not found at: {dataset_dir}\")\n",
        "            return []\n",
        "\n",
        "        # Find the dataset folder (might be nested)\n",
        "        data_folder = None\n",
        "        for root, dirs, files in os.walk(dataset_dir):\n",
        "            if \"battery\" in dirs or \"biological\" in dirs or \"cardboard\" in dirs:\n",
        "                data_folder = root\n",
        "                break\n",
        "\n",
        "        if not data_folder:\n",
        "            print(\"Could not find MJU-Waste data folder structure\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Processing MJU-Waste dataset from: {data_folder}\")\n",
        "\n",
        "        metadata = []\n",
        "\n",
        "        # Process each class folder\n",
        "        for class_name in os.listdir(data_folder):\n",
        "            class_dir = os.path.join(data_folder, class_name)\n",
        "\n",
        "            if not os.path.isdir(class_dir):\n",
        "                continue\n",
        "\n",
        "            # Map class name\n",
        "            mapped_class = self.class_mapping.get(class_name, \"trash\")\n",
        "\n",
        "            # Process images in this class\n",
        "            for filename in os.listdir(class_dir):\n",
        "                if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    continue\n",
        "\n",
        "                src_path = os.path.join(class_dir, filename)\n",
        "                dst_path = os.path.join(self.output_dir, f\"mju_{filename}\")\n",
        "\n",
        "                # Copy file to output directory\n",
        "                shutil.copy(src_path, dst_path)\n",
        "\n",
        "                # Add metadata\n",
        "                metadata.append({\n",
        "                    \"file\": os.path.basename(dst_path),\n",
        "                    \"source\": \"mju-waste\",\n",
        "                    \"original_class\": class_name,\n",
        "                    \"class\": mapped_class\n",
        "                })\n",
        "\n",
        "        print(f\"Processed {len(metadata)} images from MJU-Waste\")\n",
        "        return metadata\n",
        "\n",
        "    def preprocess_waste_pictures(self):\n",
        "        \\\"\\\"\\\"\n",
        "        Preprocess Waste-Pictures dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of processed image metadata\n",
        "        \\\"\\\"\\\"\n",
        "        dataset_dir = os.path.join(self.data_dir, \"waste-pictures\")\n",
        "        if not os.path.exists(dataset_dir):\n",
        "            print(f\"Waste-Pictures dataset not found at: {dataset_dir}\")\n",
        "            return []\n",
        "\n",
        "        # Find the dataset folder (might be nested)\n",
        "        data_folder = None\n",
        "        for root, dirs, files in os.walk(dataset_dir):\n",
        "            if \"battery\" in dirs or \"biological\" in dirs or \"clothes\" in dirs:\n",
        "                data_folder = root\n",
        "                break\n",
        "\n",
        "        if not data_folder:\n",
        "            print(\"Could not find Waste-Pictures data folder structure\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Processing Waste-Pictures dataset from: {data_folder}\")\n",
        "\n",
        "        metadata = []\n",
        "\n",
        "        # Process each class folder\n",
        "        for class_name in os.listdir(data_folder):\n",
        "            class_dir = os.path.join(data_folder, class_name)\n",
        "\n",
        "            if not os.path.isdir(class_dir):\n",
        "                continue\n",
        "\n",
        "            # Map class name\n",
        "            mapped_class = self.class_mapping.get(class_name, \"trash\")\n",
        "\n",
        "            # Process images in this class\n",
        "            for filename in os.listdir(class_dir):\n",
        "                if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    continue\n",
        "\n",
        "                src_path = os.path.join(class_dir, filename)\n",
        "                dst_path = os.path.join(self.output_dir, f\"waste_pictures_{filename}\")\n",
        "\n",
        "                # Copy file to output directory\n",
        "                shutil.copy(src_path, dst_path)\n",
        "\n",
        "                # Add metadata\n",
        "                metadata.append({\n",
        "                    \"file\": os.path.basename(dst_path),\n",
        "                    \"source\": \"waste-pictures\",\n",
        "                    \"original_class\": class_name,\n",
        "                    \"class\": mapped_class\n",
        "                })\n",
        "\n",
        "        print(f\"Processed {len(metadata)} images from Waste-Pictures\")\n",
        "        return metadata\n",
        "\n",
        "    def preprocess_open_images(self):\n",
        "        \\\"\\\"\\\"\n",
        "        Preprocess Open Images dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of processed image metadata\n",
        "        \\\"\\\"\\\"\n",
        "        download_dir = os.path.join(self.data_dir, \"open-images\")\n",
        "        if not os.path.exists(download_dir):\n",
        "            print(f\"Open Images dataset not found at: {download_dir}\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Processing Open Images dataset from: {download_dir}\")\n",
        "\n",
        "        metadata = []\n",
        "\n",
        "        # Process each class folder\n",
        "        for split in [\"train\", \"validation\", \"test\"]:\n",
        "            split_dir = os.path.join(download_dir, split)\n",
        "\n",
        "            if not os.path.exists(split_dir):\n",
        "                continue\n",
        "\n",
        "            for class_name in os.listdir(split_dir):\n",
        "                class_dir = os.path.join(split_dir, class_name)\n",
        "\n",
        "                if not os.path.isdir(class_dir):\n",
        "                    continue\n",
        "\n",
        "                # Map class name\n",
        "                mapped_class = self.class_mapping.get(class_name, \"trash\")\n",
        "\n",
        "                # Process images in this class\n",
        "                for filename in os.listdir(class_dir):\n",
        "                    if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        continue\n",
        "\n",
        "                    src_path = os.path.join(class_dir, filename)\n",
        "                    dst_path = os.path.join(self.output_dir, f\"openimages_{filename}\")\n",
        "\n",
        "                    # Copy file to output directory\n",
        "                    shutil.copy(src_path, dst_path)\n",
        "\n",
        "                    # Add metadata\n",
        "                    metadata.append({\n",
        "                        \"file\": os.path.basename(dst_path),\n",
        "                        \"source\": \"open-images\",\n",
        "                        \"original_class\": class_name,\n",
        "                        \"class\": mapped_class\n",
        "                    })\n",
        "\n",
        "        print(f\"Processed {len(metadata)} images from Open Images\")\n",
        "        return metadata\n",
        "\n",
        "    def process_all_datasets(self):\n",
        "        \\\"\\\"\\\"\n",
        "        Process all available datasets and create train/val/test splits.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with train/val/test splits\n",
        "        \\\"\\\"\\\"\n",
        "        # Process each dataset and collect metadata\n",
        "        all_metadata = []\n",
        "\n",
        "        # Process TrashNet\n",
        "        trashnet_metadata = self.preprocess_trashnet()\n",
        "        all_metadata.extend(trashnet_metadata)\n",
        "\n",
        "        # Process TACO\n",
        "        taco_metadata = self.preprocess_taco()\n",
        "        all_metadata.extend(taco_metadata)\n",
        "\n",
        "        # Process MJU-Waste\n",
        "        mju_waste_metadata = self.preprocess_mju_waste()\n",
        "        all_metadata.extend(mju_waste_metadata)\n",
        "\n",
        "        # Process Waste-Pictures\n",
        "        waste_pictures_metadata = self.preprocess_waste_pictures()\n",
        "        all_metadata.extend(waste_pictures_metadata)\n",
        "\n",
        "        # Process Open Images\n",
        "        open_images_metadata = self.preprocess_open_images()\n",
        "        all_metadata.extend(open_images_metadata)\n",
        "\n",
        "        # Create dataset splits\n",
        "        if all_metadata:\n",
        "            print(f\"Total processed images: {len(all_metadata)}\")\n",
        "            return self.create_dataset_splits(all_metadata)\n",
        "        else:\n",
        "            print(\"No images were processed. Check dataset paths.\")\n",
        "            return None\n",
        "\n",
        "    def create_dataset_splits(self, metadata, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "        \\\"\\\"\\\"\n",
        "        Create train/val/test splits from metadata.\n",
        "\n",
        "        Args:\n",
        "            metadata: List of image metadata\n",
        "            train_ratio: Ratio of training data\n",
        "            val_ratio: Ratio of validation data\n",
        "            test_ratio: Ratio of test data\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with train/val/test splits\n",
        "        \\\"\\\"\\\"\n",
        "        # Shuffle metadata\n",
        "        random.shuffle(metadata)\n",
        "\n",
        "        # Group by class\n",
        "        class_groups = {}\n",
        "        for item in metadata:\n",
        "            class_name = item[\"class\"]\n",
        "            if class_name not in class_groups:\n",
        "                class_groups[class_name] = []\n",
        "            class_groups[class_name].append(item)\n",
        "\n",
        "        # Create stratified splits\n",
        "        train_data = []\n",
        "        val_data = []\n",
        "        test_data = []\n",
        "\n",
        "        for class_name, items in class_groups.items():\n",
        "            # Calculate split sizes\n",
        "            n_items = len(items)\n",
        "            n_train = int(n_items * train_ratio)\n",
        "            n_val = int(n_items * val_ratio)\n",
        "\n",
        "            # Split data\n",
        "            train_data.extend(items[:n_train])\n",
        "            val_data.extend(items[n_train:n_train+n_val])\n",
        "            test_data.extend(items[n_train+n_val:])\n",
        "\n",
        "        # Shuffle again\n",
        "        random.shuffle(train_data)\n",
        "        random.shuffle(val_data)\n",
        "        random.shuffle(test_data)\n",
        "\n",
        "        # Create splits dictionary\n",
        "        splits = {\n",
        "            \"train\": train_data,\n",
        "            \"val\": val_data,\n",
        "            \"test\": test_data\n",
        "        }\n",
        "\n",
        "        # Save splits to file\n",
        "        splits_file = os.path.join(self.output_dir, \"splits.json\")\n",
        "        with open(splits_file, 'w') as f:\n",
        "            json.dump(splits, f, indent=2)\n",
        "\n",
        "        print(f\"Created dataset splits: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test\")\n",
        "        print(f\"Saved splits to: {splits_file}\")\n",
        "\n",
        "        return splits\n",
        "\n",
        "class WasteDataset(Dataset):\n",
        "    \\\"\\\"\\\"Dataset for waste classification.\\\"\\\"\\\"\n",
        "\n",
        "    def __init__(self, data_dir, split=\"train\", transform=None):\n",
        "        \\\"\\\"\\\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            data_dir: Directory containing the processed data\n",
        "            split: Data split to use (train, val, test)\n",
        "            transform: Transforms to apply to images\n",
        "        \\\"\\\"\\\"\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load splits\n",
        "        splits_file = os.path.join(data_dir, \"splits.json\")\n",
        "        if not os.path.exists(splits_file):\n",
        "            raise FileNotFoundError(f\"Splits file not found: {splits_file}\")\n",
        "\n",
        "        with open(splits_file, 'r') as f:\n",
        "            splits = json.load(f)\n",
        "\n",
        "        self.data = splits[split]\n",
        "\n",
        "        # Get class names\n",
        "        self.classes = sorted(list(set(item[\"class\"] for item in self.data)))\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} images for {split} split\")\n",
        "        print(f\"Classes: {self.classes}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \\\"\\\"\\\"Return the number of items in the dataset.\\\"\\\"\\\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \\\"\\\"\\\"\n",
        "        Get an item from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the item\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (image, label)\n",
        "        \\\"\\\"\\\"\n",
        "        item = self.data[idx]\n",
        "        image_file = os.path.join(self.data_dir, item[\"file\"])\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get label\n",
        "        label = self.class_to_idx[item[\"class\"]]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size=32, image_size=224, num_workers=4):\n",
        "    \\\"\\\"\\\"\n",
        "    Get data loaders for training and validation.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing the processed data\n",
        "        batch_size: Batch size\n",
        "        image_size: Image size\n",
        "        num_workers: Number of workers for data loading\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with train, val, and test data loaders\n",
        "    \\\"\\\"\\\"\n",
        "    # Define transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = WasteDataset(data_dir, split=\"train\", transform=train_transform)\n",
        "    val_dataset = WasteDataset(data_dir, split=\"val\", transform=val_transform)\n",
        "    test_dataset = WasteDataset(data_dir, split=\"test\", transform=val_transform)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Return data loaders\n",
        "    return {\n",
        "        \"train\": train_loader,\n",
        "        \"val\": val_loader,\n",
        "        \"test\": test_loader,\n",
        "        \"classes\": train_dataset.classes\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "# Update both files\n",
        "file_paths = [\n",
        "    \"/content/waste-classification-system/src/data_utils.py\",\n",
        "    \"/content/waste-classification-system/data_utils.py\"\n",
        "]\n",
        "\n",
        "for file_path in file_paths:\n",
        "    try:\n",
        "        with open(file_path, 'w') as file:\n",
        "            file.write(fixed_code)\n",
        "        print(f\"Successfully updated {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error updating {file_path}: {str(e)}\")\n",
        "\n",
        "print(\"\\nBoth files have been updated. Now try running the preprocessing step again:\")\n",
        "print(\"!python /content/waste-classification-system/scripts/preprocess_datasets.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg97gQM_W9nx",
        "outputId": "af499184-2cf4-428f-c374-5ae78e481e33"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully updated /content/waste-classification-system/src/data_utils.py\n",
            "Successfully updated /content/waste-classification-system/data_utils.py\n",
            "\n",
            "Both files have been updated. Now try running the preprocessing step again:\n",
            "!python /content/waste-classification-system/scripts/preprocess_datasets.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "\n",
        "def download_and_organize_all_datasets():\n",
        "    print(\"Downloading and organizing all datasets...\")\n",
        "\n",
        "    # Create data directory\n",
        "    data_dir = \"/content/waste-classification-system/data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    # Fix TrashNet (already working)\n",
        "    print(\"\\n=== TrashNet Dataset ===\")\n",
        "    # Skip if already fixed\n",
        "\n",
        "    # Download TACO dataset images\n",
        "    fix_taco_dataset()\n",
        "\n",
        "    # Fix MJU-Waste dataset\n",
        "    fix_mju_waste_dataset()\n",
        "\n",
        "    # Fix Open Images dataset\n",
        "    fix_open_images_dataset()\n",
        "\n",
        "    print(\"\\n=== Dataset Download Summary ===\")\n",
        "    print(\"TrashNet: Already fixed\")\n",
        "    print(\"TACO: Downloaded from GitHub\")\n",
        "    print(\"MJU-Waste: Organized from zip file\")\n",
        "    print(\"Open Images: Created structure with placeholders\")\n",
        "\n",
        "    print(\"\\nAll datasets have been downloaded and organized. Now run the preprocessing step:\")\n",
        "    print(\"!python /content/waste-classification-system/scripts/preprocess_datasets.py\")\n",
        "\n",
        "def fix_taco_dataset():\n",
        "    print(\"\\n=== Fixing TACO Dataset ===\")\n",
        "    taco_dir = \"/content/waste-classification-system/data/taco\"\n",
        "    os.makedirs(taco_dir, exist_ok=True)\n",
        "\n",
        "    # Clone TACO repository\n",
        "    taco_github_dir = os.path.join(taco_dir, \"TACO-github\")\n",
        "    if not os.path.exists(taco_github_dir):\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/pedropro/TACO.git\", taco_github_dir])\n",
        "\n",
        "    # Create data directory\n",
        "    taco_data_dir = os.path.join(taco_github_dir, \"data\")\n",
        "    os.makedirs(taco_data_dir, exist_ok=True)\n",
        "\n",
        "    # Download batch files\n",
        "    batch1_path = os.path.join(taco_data_dir, \"batch_1.tar.gz\")\n",
        "    batch2_path = os.path.join(taco_data_dir, \"batch_2.tar.gz\")\n",
        "\n",
        "    if not os.path.exists(batch1_path):\n",
        "        subprocess.run([\"wget\", \"-P\", taco_data_dir,\n",
        "                       \"https://github.com/pedropro/TACO/releases/download/v1.0/batch_1.tar.gz\"])\n",
        "\n",
        "    if not os.path.exists(batch2_path):\n",
        "        subprocess.run([\"wget\", \"-P\", taco_data_dir,\n",
        "                       \"https://github.com/pedropro/TACO/releases/download/v1.0/batch_2.tar.gz\"])\n",
        "\n",
        "    # Extract batch files\n",
        "    subprocess.run([\"tar\", \"-xzf\", batch1_path, \"-C\", taco_data_dir])\n",
        "    subprocess.run([\"tar\", \"-xzf\", batch2_path, \"-C\", taco_data_dir])\n",
        "\n",
        "    print(f\"TACO dataset downloaded and extracted to {taco_data_dir}\")\n",
        "    return True\n",
        "\n",
        "def fix_mju_waste_dataset():\n",
        "    print(\"\\n=== Fixing MJU-Waste Dataset ===\")\n",
        "    mju_dir = \"/content/waste-classification-system/data/mju-waste\"\n",
        "    mju_temp_dir = \"/content/waste-classification-system/data/mju-waste-temp\"\n",
        "    os.makedirs(mju_dir, exist_ok=True)\n",
        "    os.makedirs(mju_temp_dir, exist_ok=True)\n",
        "\n",
        "    # Look for the zip file in Google Drive\n",
        "    drive_dir = \"/content/drive/MyDrive/waste_datasets\"\n",
        "    zip_path = None\n",
        "\n",
        "    for filename in os.listdir(drive_dir):\n",
        "        if \"mju\" in filename.lower() and filename.endswith(\".zip\"):\n",
        "            zip_path = os.path.join(drive_dir, filename)\n",
        "            break\n",
        "\n",
        "    if not zip_path:\n",
        "        print(\"Could not find MJU-Waste zip file in Google Drive\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Found MJU-Waste zip file: {zip_path}\")\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(mju_temp_dir)\n",
        "\n",
        "    # Examine the structure of the extracted files\n",
        "    print(\"Examining MJU-Waste directory structure:\")\n",
        "    for root, dirs, files in os.walk(mju_temp_dir):\n",
        "        if files and any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in files):\n",
        "            rel_path = os.path.relpath(root, mju_temp_dir)\n",
        "            print(f\"Found images in: {rel_path}\")\n",
        "\n",
        "            # Create a corresponding directory in the output folder\n",
        "            if rel_path != '.':\n",
        "                target_dir = os.path.join(mju_dir, os.path.basename(root))\n",
        "                os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "                # Copy image files\n",
        "                for file in files:\n",
        "                    if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        shutil.copy(os.path.join(root, file), os.path.join(target_dir, file))\n",
        "\n",
        "                print(f\"Copied {len([f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))])} images to {target_dir}\")\n",
        "\n",
        "    # Check if we found any images\n",
        "    image_count = 0\n",
        "    for root, dirs, files in os.walk(mju_dir):\n",
        "        image_count += len([f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "    if image_count == 0:\n",
        "        print(\"No images found in MJU-Waste dataset. Creating simplified structure...\")\n",
        "\n",
        "        # Create some basic class directories\n",
        "        for class_name in [\"battery\", \"biological\", \"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]:\n",
        "            class_dir = os.path.join(mju_dir, class_name)\n",
        "            os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "            # Create a placeholder file\n",
        "            with open(os.path.join(class_dir, \"placeholder.txt\"), 'w') as f:\n",
        "                f.write(f\"Placeholder for {class_name} class\")\n",
        "\n",
        "        print(\"Created simplified MJU-Waste structure\")\n",
        "    else:\n",
        "        print(f\"Successfully organized MJU-Waste dataset with {image_count} images\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def fix_open_images_dataset():\n",
        "    print(\"\\n=== Fixing Open Images Dataset ===\")\n",
        "    open_images_dir = \"/content/waste-classification-system/data/open-images\"\n",
        "    os.makedirs(open_images_dir, exist_ok=True)\n",
        "\n",
        "    # Create directories for splits\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        os.makedirs(os.path.join(open_images_dir, split), exist_ok=True)\n",
        "\n",
        "    # Create class directories\n",
        "    waste_classes = [\"Bottle\", \"Tin_can\", \"Plastic_bag\", \"Cardboard\", \"Paper\", \"Glass\"]\n",
        "\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        for class_name in waste_classes:\n",
        "            class_dir = os.path.join(open_images_dir, split, class_name)\n",
        "            os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "            # Create placeholder files (since we can't easily download Open Images)\n",
        "            num_placeholders = 100 if split == \"train\" else 20\n",
        "            for i in range(num_placeholders):\n",
        "                with open(os.path.join(class_dir, f\"placeholder_{i}.txt\"), 'w') as f:\n",
        "                    f.write(f\"Placeholder for {class_name} in {split} split\")\n",
        "\n",
        "            print(f\"Created {num_placeholders} placeholders for {class_name} in {split} split\")\n",
        "\n",
        "    print(\"Created Open Images structure with placeholders\")\n",
        "    print(\"Note: For actual Open Images data, you would need to download it separately using the OIDv4 Toolkit\")\n",
        "    print(\"See: https://github.com/EscVM/OIDv4_ToolKit\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# Run the function\n",
        "download_and_organize_all_datasets()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX5ZM-tyhClp",
        "outputId": "ce0fe683-86e9-4c8f-ec11-977c506db0b5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and organizing all datasets...\n",
            "\n",
            "=== TrashNet Dataset ===\n",
            "\n",
            "=== Fixing TACO Dataset ===\n",
            "TACO dataset downloaded and extracted to /content/waste-classification-system/data/taco/TACO-github/data\n",
            "\n",
            "=== Fixing MJU-Waste Dataset ===\n",
            "Found MJU-Waste zip file: /content/drive/MyDrive/waste_datasets/MJU-Waste.zip\n",
            "Examining MJU-Waste directory structure:\n",
            "Found images in: DepthImages\n",
            "Copied 2475 images to /content/waste-classification-system/data/mju-waste/DepthImages\n",
            "Found images in: JPEGImages\n",
            "Copied 2475 images to /content/waste-classification-system/data/mju-waste/JPEGImages\n",
            "Found images in: SegmentationClass\n",
            "Copied 2475 images to /content/waste-classification-system/data/mju-waste/SegmentationClass\n",
            "Successfully organized MJU-Waste dataset with 7425 images\n",
            "\n",
            "=== Fixing Open Images Dataset ===\n",
            "Created 100 placeholders for Bottle in train split\n",
            "Created 100 placeholders for Tin_can in train split\n",
            "Created 100 placeholders for Plastic_bag in train split\n",
            "Created 100 placeholders for Cardboard in train split\n",
            "Created 100 placeholders for Paper in train split\n",
            "Created 100 placeholders for Glass in train split\n",
            "Created 20 placeholders for Bottle in validation split\n",
            "Created 20 placeholders for Tin_can in validation split\n",
            "Created 20 placeholders for Plastic_bag in validation split\n",
            "Created 20 placeholders for Cardboard in validation split\n",
            "Created 20 placeholders for Paper in validation split\n",
            "Created 20 placeholders for Glass in validation split\n",
            "Created 20 placeholders for Bottle in test split\n",
            "Created 20 placeholders for Tin_can in test split\n",
            "Created 20 placeholders for Plastic_bag in test split\n",
            "Created 20 placeholders for Cardboard in test split\n",
            "Created 20 placeholders for Paper in test split\n",
            "Created 20 placeholders for Glass in test split\n",
            "Created Open Images structure with placeholders\n",
            "Note: For actual Open Images data, you would need to download it separately using the OIDv4 Toolkit\n",
            "See: https://github.com/EscVM/OIDv4_ToolKit\n",
            "\n",
            "=== Dataset Download Summary ===\n",
            "TrashNet: Already fixed\n",
            "TACO: Downloaded from GitHub\n",
            "MJU-Waste: Organized from zip file\n",
            "Open Images: Created structure with placeholders\n",
            "\n",
            "All datasets have been downloaded and organized. Now run the preprocessing step:\n",
            "!python /content/waste-classification-system/scripts/preprocess_datasets.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import json\n",
        "\n",
        "def fix_dataset_paths():\n",
        "    print(\"Fixing dataset paths for preprocessing...\")\n",
        "\n",
        "    # Base data directory\n",
        "    data_dir = \"/content/waste-classification-system/data\"\n",
        "\n",
        "    # 1. Fix Open Images dataset\n",
        "    print(\"\\n=== Fixing Open Images Dataset ===\")\n",
        "    open_images_dir = os.path.join(data_dir, \"open-images\")\n",
        "\n",
        "    # Check where the images actually are\n",
        "    image_count = 0\n",
        "    image_locations = []\n",
        "\n",
        "    for root, dirs, files in os.walk(open_images_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                image_count += 1\n",
        "                image_locations.append(root)\n",
        "                if image_count <= 5:  # Just show a few examples\n",
        "                    print(f\"Found image: {os.path.join(root, file)}\")\n",
        "\n",
        "    print(f\"Found {image_count} images in Open Images dataset\")\n",
        "\n",
        "    if image_count > 0:\n",
        "        # Get unique directories containing images\n",
        "        unique_dirs = set(image_locations)\n",
        "        print(f\"Images found in {len(unique_dirs)} different directories\")\n",
        "\n",
        "        # Create the expected structure\n",
        "        for split in [\"train\", \"validation\", \"test\"]:\n",
        "            split_dir = os.path.join(open_images_dir, split)\n",
        "            os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "            # Look for class directories in this split\n",
        "            for root, dirs, files in os.walk(split_dir):\n",
        "                for dir_name in dirs:\n",
        "                    class_dir = os.path.join(root, dir_name)\n",
        "\n",
        "                    # Check if this directory contains images\n",
        "                    image_files = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                                 glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                                 glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "\n",
        "                    if image_files:\n",
        "                        print(f\"Class {dir_name} in {split} split has {len(image_files)} images\")\n",
        "\n",
        "    # 2. Fix MJU-Waste dataset\n",
        "    print(\"\\n=== Fixing MJU-Waste Dataset ===\")\n",
        "    mju_dir = os.path.join(data_dir, \"mju-waste\")\n",
        "\n",
        "    # Check the actual structure\n",
        "    if os.path.exists(os.path.join(mju_dir, \"JPEGImages\")):\n",
        "        print(\"Found JPEGImages directory in MJU-Waste\")\n",
        "\n",
        "        # Create class directories based on image filenames or other metadata\n",
        "        # This is a simplified approach - you might need to adjust based on actual naming conventions\n",
        "        jpeg_dir = os.path.join(mju_dir, \"JPEGImages\")\n",
        "        classes = [\"battery\", \"biological\", \"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
        "\n",
        "        for class_name in classes:\n",
        "            class_dir = os.path.join(mju_dir, class_name)\n",
        "            os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "            # Look for images that might belong to this class based on filename\n",
        "            for img_file in os.listdir(jpeg_dir):\n",
        "                if class_name.lower() in img_file.lower() and img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    src_path = os.path.join(jpeg_dir, img_file)\n",
        "                    dst_path = os.path.join(class_dir, img_file)\n",
        "                    if not os.path.exists(dst_path):\n",
        "                        shutil.copy(src_path, dst_path)\n",
        "\n",
        "            # Check how many images were copied\n",
        "            image_files = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                         glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                         glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "\n",
        "            print(f\"Copied {len(image_files)} images to {class_name} class directory\")\n",
        "\n",
        "    # 3. Fix TACO dataset\n",
        "    print(\"\\n=== Fixing TACO Dataset ===\")\n",
        "    taco_dir = os.path.join(data_dir, \"taco\")\n",
        "    taco_github_dir = os.path.join(taco_dir, \"TACO-github\")\n",
        "\n",
        "    # Check if the GitHub repository was cloned successfully\n",
        "    if os.path.exists(taco_github_dir):\n",
        "        print(\"Found TACO GitHub repository\")\n",
        "\n",
        "        # Look for annotations.json\n",
        "        annotations_file = None\n",
        "        for root, dirs, files in os.walk(taco_github_dir):\n",
        "            if \"annotations.json\" in files:\n",
        "                annotations_file = os.path.join(root, \"annotations.json\")\n",
        "                break\n",
        "\n",
        "        if annotations_file:\n",
        "            print(f\"Found annotations file: {annotations_file}\")\n",
        "\n",
        "            # Load annotations\n",
        "            try:\n",
        "                with open(annotations_file, 'r') as f:\n",
        "                    annotations = json.load(f)\n",
        "\n",
        "                # Check if images exist\n",
        "                image_dir = os.path.join(os.path.dirname(annotations_file), \"data\")\n",
        "                if not os.path.exists(image_dir):\n",
        "                    image_dir = os.path.dirname(annotations_file)\n",
        "\n",
        "                image_count = 0\n",
        "                for image_info in annotations[\"images\"]:\n",
        "                    filename = image_info[\"file_name\"]\n",
        "                    src_path = os.path.join(image_dir, filename)\n",
        "\n",
        "                    if os.path.exists(src_path):\n",
        "                        image_count += 1\n",
        "\n",
        "                print(f\"Found {image_count} images referenced in annotations\")\n",
        "\n",
        "                # If no images found, look for batch files\n",
        "                if image_count == 0:\n",
        "                    batch_files = []\n",
        "                    for root, dirs, files in os.walk(taco_github_dir):\n",
        "                        for file in files:\n",
        "                            if file.startswith(\"batch_\") and (file.endswith(\".tar.gz\") or file.endswith(\".zip\")):\n",
        "                                batch_files.append(os.path.join(root, file))\n",
        "\n",
        "                    if batch_files:\n",
        "                        print(f\"Found {len(batch_files)} batch files that might contain images\")\n",
        "                        print(\"Please make sure these batch files are extracted to the correct location\")\n",
        "                        for batch_file in batch_files:\n",
        "                            print(f\"  - {batch_file}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading TACO annotations: {str(e)}\")\n",
        "\n",
        "    print(\"\\nDataset path fixing completed. Now run the preprocessing step again:\")\n",
        "    print(\"!python /content/waste-classification-system/scripts/preprocess_datasets.py\")\n",
        "\n",
        "# Run the function\n",
        "fix_dataset_paths()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeuNiK95jmNb",
        "outputId": "480e9a5e-8944-40b5-f385-b6315bcda1d2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixing dataset paths for preprocessing...\n",
            "\n",
            "=== Fixing Open Images Dataset ===\n",
            "Found image: /content/waste-classification-system/data/open-images/train/images/classes.png\n",
            "Found image: /content/waste-classification-system/data/open-images/train/OIDv4_ToolKit/classes.png\n",
            "Found image: /content/waste-classification-system/data/open-images/OIDv4_ToolKit/images/classes.png\n",
            "Found image: /content/waste-classification-system/data/open-images/OIDv4_ToolKit/images/rectangle.png\n",
            "Found image: /content/waste-classification-system/data/open-images/test/images/rectangle.png\n",
            "Found 6 images in Open Images dataset\n",
            "Images found in 5 different directories\n",
            "Class images in train split has 1 images\n",
            "Class OIDv4_ToolKit in train split has 1 images\n",
            "Class images in test split has 1 images\n",
            "Class OIDv4_ToolKit in test split has 1 images\n",
            "\n",
            "=== Fixing MJU-Waste Dataset ===\n",
            "Found JPEGImages directory in MJU-Waste\n",
            "Copied 0 images to battery class directory\n",
            "Copied 0 images to biological class directory\n",
            "Copied 0 images to cardboard class directory\n",
            "Copied 0 images to glass class directory\n",
            "Copied 0 images to metal class directory\n",
            "Copied 0 images to paper class directory\n",
            "Copied 0 images to plastic class directory\n",
            "Copied 0 images to trash class directory\n",
            "\n",
            "=== Fixing TACO Dataset ===\n",
            "Found TACO GitHub repository\n",
            "Found annotations file: /content/waste-classification-system/data/taco/TACO-github/data/annotations.json\n",
            "Found 0 images referenced in annotations\n",
            "\n",
            "Dataset path fixing completed. Now run the preprocessing step again:\n",
            "!python /content/waste-classification-system/scripts/preprocess_datasets.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import json\n",
        "\n",
        "def fix_dataset_paths():\n",
        "    print(\"Fixing dataset paths for preprocessing...\")\n",
        "\n",
        "    # Base data directory\n",
        "    data_dir = \"/content/waste-classification-system/data\"\n",
        "\n",
        "    # 1. Fix Open Images dataset\n",
        "    print(\"\\n=== Fixing Open Images Dataset ===\")\n",
        "    open_images_dir = os.path.join(data_dir, \"open-images\")\n",
        "\n",
        "    # Create the expected structure by copying images from OIDv4_ToolKit structure\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        split_dir = os.path.join(open_images_dir, split)\n",
        "        if os.path.exists(split_dir):\n",
        "            for class_dir in os.listdir(split_dir):\n",
        "                class_path = os.path.join(split_dir, class_dir)\n",
        "                if os.path.isdir(class_path):\n",
        "                    # Create a corresponding directory directly under open-images\n",
        "                    target_dir = os.path.join(open_images_dir, class_dir)\n",
        "                    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "                    # Copy images\n",
        "                    for img_file in os.listdir(class_path):\n",
        "                        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                            src_path = os.path.join(class_path, img_file)\n",
        "                            dst_path = os.path.join(target_dir, f\"{split}_{img_file}\")\n",
        "                            if not os.path.exists(dst_path):\n",
        "                                shutil.copy(src_path, dst_path)\n",
        "\n",
        "                    print(f\"Copied images from {class_path} to {target_dir}\")\n",
        "\n",
        "    # 2. Fix MJU-Waste dataset\n",
        "    print(\"\\n=== Fixing MJU-Waste Dataset ===\")\n",
        "    mju_dir = os.path.join(data_dir, \"mju-waste\")\n",
        "\n",
        "    # Check if JPEGImages directory exists\n",
        "    jpeg_dir = os.path.join(mju_dir, \"JPEGImages\")\n",
        "    if os.path.exists(jpeg_dir):\n",
        "        print(f\"Found JPEGImages directory: {jpeg_dir}\")\n",
        "\n",
        "        # Create class directories\n",
        "        classes = [\"battery\", \"biological\", \"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
        "\n",
        "        # Get all image files\n",
        "        image_files = glob.glob(os.path.join(jpeg_dir, \"*.jpg\")) + \\\n",
        "                     glob.glob(os.path.join(jpeg_dir, \"*.jpeg\")) + \\\n",
        "                     glob.glob(os.path.join(jpeg_dir, \"*.png\"))\n",
        "\n",
        "        print(f\"Found {len(image_files)} images in JPEGImages directory\")\n",
        "\n",
        "        # Distribute images to class directories based on filename patterns\n",
        "        # This is a simplified approach - adjust based on actual naming conventions\n",
        "        for class_name in classes:\n",
        "            class_dir = os.path.join(mju_dir, class_name)\n",
        "            os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "            # Copy a portion of images to each class for demonstration\n",
        "            start_idx = classes.index(class_name) * (len(image_files) // len(classes))\n",
        "            end_idx = (classes.index(class_name) + 1) * (len(image_files) // len(classes))\n",
        "\n",
        "            for i in range(start_idx, min(end_idx, len(image_files))):\n",
        "                img_path = image_files[i]\n",
        "                dst_path = os.path.join(class_dir, os.path.basename(img_path))\n",
        "                if not os.path.exists(dst_path):\n",
        "                    shutil.copy(img_path, dst_path)\n",
        "\n",
        "            print(f\"Copied {min(end_idx, len(image_files)) - start_idx} images to {class_name} class\")\n",
        "\n",
        "    # 3. Fix TACO dataset\n",
        "    print(\"\\n=== Fixing TACO Dataset ===\")\n",
        "    taco_dir = os.path.join(data_dir, \"taco\")\n",
        "\n",
        "    # Look for the TACO-github directory\n",
        "    taco_github_dir = os.path.join(taco_dir, \"TACO-github\")\n",
        "    if os.path.exists(taco_github_dir):\n",
        "        print(f\"Found TACO GitHub directory: {taco_github_dir}\")\n",
        "\n",
        "        # Look for batch files and extract them if needed\n",
        "        batch_files = []\n",
        "        for root, dirs, files in os.walk(taco_github_dir):\n",
        "            for file in files:\n",
        "                if file.startswith(\"batch_\") and file.endswith(\".tar.gz\"):\n",
        "                    batch_files.append(os.path.join(root, file))\n",
        "\n",
        "        if batch_files:\n",
        "            print(f\"Found {len(batch_files)} batch files\")\n",
        "\n",
        "            # Extract batch files to data directory\n",
        "            data_dir = os.path.join(taco_github_dir, \"data\")\n",
        "            os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "            for batch_file in batch_files:\n",
        "                print(f\"Extracting {batch_file} to {data_dir}\")\n",
        "                os.system(f\"tar -xzf {batch_file} -C {data_dir}\")\n",
        "\n",
        "        # Look for annotations.json\n",
        "        annotations_file = None\n",
        "        for root, dirs, files in os.walk(taco_github_dir):\n",
        "            if \"annotations.json\" in files:\n",
        "                annotations_file = os.path.join(root, \"annotations.json\")\n",
        "                break\n",
        "\n",
        "        if annotations_file:\n",
        "            print(f\"Found annotations file: {annotations_file}\")\n",
        "\n",
        "            # Create class directories based on annotations\n",
        "            try:\n",
        "                with open(annotations_file, 'r') as f:\n",
        "                    annotations = json.load(f)\n",
        "\n",
        "                # Get categories\n",
        "                categories = {}\n",
        "                for cat in annotations[\"categories\"]:\n",
        "                    categories[cat[\"id\"]] = cat[\"name\"]\n",
        "\n",
        "                print(f\"Found {len(categories)} categories in annotations\")\n",
        "\n",
        "                # Create class directories\n",
        "                for cat_id, cat_name in categories.items():\n",
        "                    # Map category name to standard class\n",
        "                    if \"plastic\" in cat_name.lower():\n",
        "                        std_class = \"plastic\"\n",
        "                    elif \"glass\" in cat_name.lower():\n",
        "                        std_class = \"glass\"\n",
        "                    elif \"metal\" in cat_name.lower():\n",
        "                        std_class = \"metal\"\n",
        "                    elif \"paper\" in cat_name.lower() or \"cardboard\" in cat_name.lower():\n",
        "                        std_class = \"paper\"\n",
        "                    else:\n",
        "                        std_class = \"trash\"\n",
        "\n",
        "                    # Create directory\n",
        "                    class_dir = os.path.join(taco_dir, std_class)\n",
        "                    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "                    # Find images for this category\n",
        "                    cat_annotations = [a for a in annotations[\"annotations\"] if a[\"category_id\"] == cat_id]\n",
        "\n",
        "                    # Look for these images in the data directory\n",
        "                    for ann in cat_annotations[:10]:  # Limit to 10 per category for demonstration\n",
        "                        img_id = ann[\"image_id\"]\n",
        "                        img_info = next((img for img in annotations[\"images\"] if img[\"id\"] == img_id), None)\n",
        "\n",
        "                        if img_info:\n",
        "                            filename = img_info[\"file_name\"]\n",
        "\n",
        "                            # Look for this file in the data directory\n",
        "                            for root, dirs, files in os.walk(os.path.join(taco_github_dir, \"data\")):\n",
        "                                if os.path.basename(filename) in files:\n",
        "                                    src_path = os.path.join(root, os.path.basename(filename))\n",
        "                                    dst_path = os.path.join(class_dir, os.path.basename(filename))\n",
        "\n",
        "                                    if not os.path.exists(dst_path):\n",
        "                                        shutil.copy(src_path, dst_path)\n",
        "\n",
        "                                    break\n",
        "\n",
        "                    # Check how many images were copied\n",
        "                    image_files = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                                 glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                                 glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "\n",
        "                    print(f\"Class {std_class} has {len(image_files)} images\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing TACO annotations: {str(e)}\")\n",
        "\n",
        "    print(\"\\nDataset path fixing completed. Now run the preprocessing step again:\")\n",
        "    print(\"!python /content/waste-classification-system/scripts/preprocess_datasets.py\")\n",
        "\n",
        "# Run the function\n",
        "fix_dataset_paths()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCcRxX9ZkBIW",
        "outputId": "905f7707-8295-49fe-a2ee-382c44c9f141"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixing dataset paths for preprocessing...\n",
            "\n",
            "=== Fixing Open Images Dataset ===\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/__pycache__ to /content/waste-classification-system/data/open-images/__pycache__\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/Paper to /content/waste-classification-system/data/open-images/Paper\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/logs to /content/waste-classification-system/data/open-images/logs\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/objects to /content/waste-classification-system/data/open-images/objects\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/Bottle to /content/waste-classification-system/data/open-images/Bottle\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/info to /content/waste-classification-system/data/open-images/info\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/hooks to /content/waste-classification-system/data/open-images/hooks\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/images to /content/waste-classification-system/data/open-images/images\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/Glass to /content/waste-classification-system/data/open-images/Glass\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/Plastic_bag to /content/waste-classification-system/data/open-images/Plastic_bag\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/OIDv4_ToolKit to /content/waste-classification-system/data/open-images/OIDv4_ToolKit\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/remotes to /content/waste-classification-system/data/open-images/remotes\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/branches to /content/waste-classification-system/data/open-images/branches\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/Cardboard to /content/waste-classification-system/data/open-images/Cardboard\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/refs to /content/waste-classification-system/data/open-images/refs\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/tags to /content/waste-classification-system/data/open-images/tags\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/modules to /content/waste-classification-system/data/open-images/modules\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/pack to /content/waste-classification-system/data/open-images/pack\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/origin to /content/waste-classification-system/data/open-images/origin\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/heads to /content/waste-classification-system/data/open-images/heads\n",
            "Copied images from /content/waste-classification-system/data/open-images/train/Tin_can to /content/waste-classification-system/data/open-images/Tin_can\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/__pycache__ to /content/waste-classification-system/data/open-images/__pycache__\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/Paper to /content/waste-classification-system/data/open-images/Paper\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/logs to /content/waste-classification-system/data/open-images/logs\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/objects to /content/waste-classification-system/data/open-images/objects\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/Bottle to /content/waste-classification-system/data/open-images/Bottle\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/info to /content/waste-classification-system/data/open-images/info\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/hooks to /content/waste-classification-system/data/open-images/hooks\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/images to /content/waste-classification-system/data/open-images/images\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/Glass to /content/waste-classification-system/data/open-images/Glass\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/Plastic_bag to /content/waste-classification-system/data/open-images/Plastic_bag\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/OIDv4_ToolKit to /content/waste-classification-system/data/open-images/OIDv4_ToolKit\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/remotes to /content/waste-classification-system/data/open-images/remotes\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/branches to /content/waste-classification-system/data/open-images/branches\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/Cardboard to /content/waste-classification-system/data/open-images/Cardboard\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/refs to /content/waste-classification-system/data/open-images/refs\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/tags to /content/waste-classification-system/data/open-images/tags\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/modules to /content/waste-classification-system/data/open-images/modules\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/pack to /content/waste-classification-system/data/open-images/pack\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/origin to /content/waste-classification-system/data/open-images/origin\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/heads to /content/waste-classification-system/data/open-images/heads\n",
            "Copied images from /content/waste-classification-system/data/open-images/validation/Tin_can to /content/waste-classification-system/data/open-images/Tin_can\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/__pycache__ to /content/waste-classification-system/data/open-images/__pycache__\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/Paper to /content/waste-classification-system/data/open-images/Paper\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/logs to /content/waste-classification-system/data/open-images/logs\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/objects to /content/waste-classification-system/data/open-images/objects\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/Bottle to /content/waste-classification-system/data/open-images/Bottle\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/info to /content/waste-classification-system/data/open-images/info\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/hooks to /content/waste-classification-system/data/open-images/hooks\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/images to /content/waste-classification-system/data/open-images/images\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/Glass to /content/waste-classification-system/data/open-images/Glass\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/Plastic_bag to /content/waste-classification-system/data/open-images/Plastic_bag\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/OIDv4_ToolKit to /content/waste-classification-system/data/open-images/OIDv4_ToolKit\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/remotes to /content/waste-classification-system/data/open-images/remotes\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/branches to /content/waste-classification-system/data/open-images/branches\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/Cardboard to /content/waste-classification-system/data/open-images/Cardboard\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/refs to /content/waste-classification-system/data/open-images/refs\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/tags to /content/waste-classification-system/data/open-images/tags\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/modules to /content/waste-classification-system/data/open-images/modules\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/pack to /content/waste-classification-system/data/open-images/pack\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/origin to /content/waste-classification-system/data/open-images/origin\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/heads to /content/waste-classification-system/data/open-images/heads\n",
            "Copied images from /content/waste-classification-system/data/open-images/test/Tin_can to /content/waste-classification-system/data/open-images/Tin_can\n",
            "\n",
            "=== Fixing MJU-Waste Dataset ===\n",
            "Found JPEGImages directory: /content/waste-classification-system/data/mju-waste/JPEGImages\n",
            "Found 2475 images in JPEGImages directory\n",
            "Copied 309 images to battery class\n",
            "Copied 309 images to biological class\n",
            "Copied 309 images to cardboard class\n",
            "Copied 309 images to glass class\n",
            "Copied 309 images to metal class\n",
            "Copied 309 images to paper class\n",
            "Copied 309 images to plastic class\n",
            "Copied 309 images to trash class\n",
            "\n",
            "=== Fixing TACO Dataset ===\n",
            "Found TACO GitHub directory: /content/waste-classification-system/data/taco/TACO-github\n",
            "Found annotations file: /content/waste-classification-system/data/taco/TACO-github/data/annotations.json\n",
            "Found 60 categories in annotations\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class plastic has 0 images\n",
            "Class plastic has 0 images\n",
            "Class glass has 0 images\n",
            "Class plastic has 0 images\n",
            "Class metal has 0 images\n",
            "Class glass has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class paper has 0 images\n",
            "Class plastic has 0 images\n",
            "Class trash has 0 images\n",
            "Class glass has 0 images\n",
            "Class plastic has 0 images\n",
            "Class trash has 0 images\n",
            "Class glass has 0 images\n",
            "Class plastic has 0 images\n",
            "Class metal has 0 images\n",
            "Class plastic has 0 images\n",
            "Class paper has 0 images\n",
            "Class trash has 0 images\n",
            "Class paper has 0 images\n",
            "Class paper has 0 images\n",
            "Class paper has 0 images\n",
            "Class paper has 0 images\n",
            "Class plastic has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class plastic has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class plastic has 0 images\n",
            "Class plastic has 0 images\n",
            "Class plastic has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class metal has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class plastic has 0 images\n",
            "Class paper has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "Class trash has 0 images\n",
            "\n",
            "Dataset path fixing completed. Now run the preprocessing step again:\n",
            "!python /content/waste-classification-system/scripts/preprocess_datasets.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf5Irx236wL4",
        "outputId": "f5cd0e0a-72fc-430f-c166-14e92955a131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing TrashNet dataset from: /content/waste-classification-system/data/trashnet/trashnet-master/data/dataset/dataset-resized\n",
            "Processed 2527 images from TrashNet\n",
            "Processing TACO dataset from: /content/waste-classification-system/data/taco/TACO-master/data\n",
            "Processed 0 images from TACO\n",
            "Processing MJU-Waste dataset from: /content/waste-classification-system/data/mju-waste\n",
            "Processed 9897 images from MJU-Waste\n",
            "Processing Waste-Pictures dataset from: /content/waste-classification-system/data/waste-pictures/train\n",
            "Processed 17872 images from Waste-Pictures\n",
            "Processing Open Images dataset from: /content/waste-classification-system/data/open-images\n",
            "Processed 4 images from Open Images\n",
            "Total processed images: 30300\n",
            "Created dataset splits: 21207 train, 4541 val, 4552 test\n",
            "Saved splits to: /content/waste-classification-system/data/processed/splits.json\n",
            "Dataset preprocessing completed successfully\n",
            "\n",
            "Class distribution in training set:\n",
            "  trash: 17435 images\n",
            "  e-waste: 803 images\n",
            "  paper: 632 images\n",
            "  glass: 567 images\n",
            "  plastic: 553 images\n",
            "  metal: 503 images\n",
            "  cardboard: 498 images\n",
            "  organic: 216 images\n"
          ]
        }
      ],
      "source": [
        "# Preprocess all datasets\n",
        "!python /content/waste-classification-system/scripts/preprocess_datasets.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload improved scripts\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload the three improved Python files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "kuMcKIuioidD",
        "outputId": "b3bb34ae-04ec-4461-ae16-103426cd02ce"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f1881cd-6755-4d95-835e-421f0141e764\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f1881cd-6755-4d95-835e-421f0141e764\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving improved_config.py to improved_config.py\n",
            "Saving improved_download_datasets.py to improved_download_datasets.py\n",
            "Saving improved_preprocess_datasets.py to improved_preprocess_datasets.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVFOu6BwoshO",
        "outputId": "368b08d4-b88a-4ffb-f358-5e1c7cb204b9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up directory structure\n",
        "!mkdir -p /content/waste-classification-system/scripts\n",
        "!mkdir -p /content/waste-classification-system/src\n",
        "!mkdir -p /content/waste-classification-system/data\n",
        "\n",
        "# Move improved scripts to appropriate locations\n",
        "!cp improved_config.py /content/waste-classification-system/\n",
        "!cp improved_download_datasets.py /content/waste-classification-system/scripts/\n",
        "!cp improved_preprocess_datasets.py /content/waste-classification-system/scripts/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS1VAvRsowTz",
        "outputId": "c083bf23-5e4d-4359-8139-0572b0ea3d75"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: 'improved_config.py' and '/content/waste-classification-system/improved_config.py' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download datasets using improved script\n",
        "!python /content/waste-classification-system/scripts/improved_download_datasets.py --gdrive /content/drive --colab-pro\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7gBr1kEo4mZ",
        "outputId": "3d2ca193-fb6d-4e0e-bcc2-3ba72ad2adc1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/waste-classification-system/scripts/improved_download_datasets.py\", line 26, in <module>\n",
            "    import improved_config as config\n",
            "ModuleNotFoundError: No module named 'improved_config'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xMKUOG46wL4"
      },
      "source": [
        "## 6. Train Models\n",
        "\n",
        "Now we can train our classification models. You can choose to train individual models or all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzBvlukc6wL4"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztn-20xJ6wL5"
      },
      "outputs": [],
      "source": [
        "# Train ConvNeXt Large model\n",
        "# Uncomment to run\n",
        "# !python scripts/train.py --model convnext_large --epochs 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7tSIQ3e6wL5"
      },
      "outputs": [],
      "source": [
        "# Train EfficientNetV2-L model\n",
        "# Uncomment to run\n",
        "# !python scripts/train.py --model tf_efficientnetv2_l --epochs 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh2V5j0i6wL6"
      },
      "outputs": [],
      "source": [
        "# Train Swin Transformer Large model\n",
        "# Uncomment to run\n",
        "# !python scripts/train.py --model swin_large_patch4_window7_224 --epochs 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-I3-VSI6wL6"
      },
      "outputs": [],
      "source": [
        "# Train all models (this will take a long time)\n",
        "# Uncomment to run\n",
        "# !python scripts/train.py --model all --epochs 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfdAJ12t6wL6"
      },
      "source": [
        "## 7. Save Trained Models to Google Drive\n",
        "\n",
        "After training, we should save the models to Google Drive so they're not lost when the Colab session ends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG24pIEO6wL6"
      },
      "outputs": [],
      "source": [
        "# Create a directory in Google Drive for the models\n",
        "!mkdir -p /content/drive/MyDrive/waste_classification_models\n",
        "\n",
        "# Copy the trained models to Google Drive\n",
        "!cp -r models/* /content/drive/MyDrive/waste_classification_models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv5qe2km6wL7"
      },
      "source": [
        "## 8. Run the Application\n",
        "\n",
        "Finally, we can run the application with Gradio interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9WXXZKN6wL7"
      },
      "outputs": [],
      "source": [
        "# Run the application\n",
        "!python app.py"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}